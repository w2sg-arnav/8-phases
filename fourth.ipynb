{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.6.0+cu118)\n",
      "Requirement already satisfied: torchvision in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.21.0+cu118)\n",
      "Requirement already satisfied: opencv-python in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.11.0.86)\n",
      "Requirement already satisfied: scikit-image in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.25.2)\n",
      "Requirement already satisfied: albumentations in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.0.5)\n",
      "Requirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: matplotlib in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (3.8.2)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (11.8.86)\n",
      "Requirement already satisfied: triton==3.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: scipy>=1.11.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-image) (1.11.4)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-image) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-image) (2025.2.18)\n",
      "Requirement already satisfied: packaging>=21 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-image) (24.2)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-image) (0.4)\n",
      "Requirement already satisfied: PyYAML in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from albumentations) (6.0.2)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from albumentations) (2.10.6)\n",
      "Requirement already satisfied: albucore==0.0.23 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from albumentations) (0.0.23)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from albumentations) (4.11.0.86)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from albucore==0.0.23->albumentations) (3.12.2)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from albucore==0.0.23->albumentations) (6.2.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic>=2.9.2->albumentations) (2.27.2)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Setup and imports complete.\n"
     ]
    }
   ],
   "source": [
    "# @title Cell 1: Setup and Imports\n",
    "\n",
    "# **Important:** Run this cell first to install necessary libraries.\n",
    "\n",
    "!pip install torch torchvision opencv-python scikit-image albumentations tqdm matplotlib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "from skimage.restoration import denoise_nl_means, denoise_bilateral, denoise_wavelet\n",
    "from skimage import img_as_float, img_as_ubyte\n",
    "from skimage.transform import rotate\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Setup and imports complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/CASIA-IVA-Lab/FastSAM.git\n",
      "  Cloning https://github.com/CASIA-IVA-Lab/FastSAM.git to /tmp/pip-req-build-q7uy3weg\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/CASIA-IVA-Lab/FastSAM.git /tmp/pip-req-build-q7uy3weg\n",
      "\n",
      "  Resolved https://github.com/CASIA-IVA-Lab/FastSAM.git to commit b4ed20c2fed75eadc5aa7d8b09fedd137b873b52\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting CLIP@ git+https://github.com/openai/CLIP.git@a1d071733d7111c9c014f024669f959182114e33#egg=CLIP (from fastsam==0.1.1)\n",
      "  Using cached clip-1.0-py3-none-any.whl\n",
      "Requirement already satisfied: matplotlib>=3.2.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastsam==0.1.1) (3.8.2)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastsam==0.1.1) (4.11.0.86)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastsam==0.1.1) (11.1.0)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastsam==0.1.1) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastsam==0.1.1) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastsam==0.1.1) (1.11.4)\n",
      "Requirement already satisfied: torch>=1.7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastsam==0.1.1) (2.6.0+cu118)\n",
      "Requirement already satisfied: torchvision>=0.8.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastsam==0.1.1) (0.21.0+cu118)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastsam==0.1.1) (4.67.1)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastsam==0.1.1) (2.1.4)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastsam==0.1.1) (0.13.2)\n",
      "Requirement already satisfied: gradio==3.35.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastsam==0.1.1) (3.35.2)\n",
      "Requirement already satisfied: aiofiles in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio==3.35.2->fastsam==0.1.1) (24.1.0)\n",
      "Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio==3.35.2->fastsam==0.1.1) (3.11.13)\n",
      "Requirement already satisfied: altair>=4.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio==3.35.2->fastsam==0.1.1) (5.5.0)\n",
      "Requirement already satisfied: fastapi in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio==3.35.2->fastsam==0.1.1) (0.115.8)\n",
      "Requirement already satisfied: ffmpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio==3.35.2->fastsam==0.1.1) (0.5.0)\n",
      "Requirement already satisfied: gradio-client>=0.2.7 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio==3.35.2->fastsam==0.1.1) (1.7.2)\n",
      "Requirement already satisfied: httpx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio==3.35.2->fastsam==0.1.1) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.14.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio==3.35.2->fastsam==0.1.1) (0.29.1)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio==3.35.2->fastsam==0.1.1) (3.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from markdown-it-py[linkify]>=2.0.0->gradio==3.35.2->fastsam==0.1.1) (2.2.0)\n",
      "Requirement already satisfied: markupsafe in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio==3.35.2->fastsam==0.1.1) (3.0.2)\n",
      "Requirement already satisfied: mdit-py-plugins<=0.3.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio==3.35.2->fastsam==0.1.1) (0.3.3)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio==3.35.2->fastsam==0.1.1) (1.26.4)\n",
      "Requirement already satisfied: orjson in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio==3.35.2->fastsam==0.1.1) (3.10.15)\n",
      "Requirement already satisfied: pydantic in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio==3.35.2->fastsam==0.1.1) (2.10.6)\n",
      "Requirement already satisfied: pydub in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio==3.35.2->fastsam==0.1.1) (0.25.1)\n",
      "Requirement already satisfied: pygments>=2.12.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio==3.35.2->fastsam==0.1.1) (2.19.1)\n",
      "Requirement already satisfied: python-multipart in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio==3.35.2->fastsam==0.1.1) (0.0.20)\n",
      "Requirement already satisfied: semantic-version in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio==3.35.2->fastsam==0.1.1) (2.10.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio==3.35.2->fastsam==0.1.1) (0.34.0)\n",
      "Requirement already satisfied: websockets>=10.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio==3.35.2->fastsam==0.1.1) (15.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib>=3.2.2->fastsam==0.1.1) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib>=3.2.2->fastsam==0.1.1) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib>=3.2.2->fastsam==0.1.1) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib>=3.2.2->fastsam==0.1.1) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib>=3.2.2->fastsam==0.1.1) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib>=3.2.2->fastsam==0.1.1) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib>=3.2.2->fastsam==0.1.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas>=1.1.4->fastsam==0.1.1) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas>=1.1.4->fastsam==0.1.1) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.23.0->fastsam==0.1.1) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.23.0->fastsam==0.1.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.23.0->fastsam==0.1.1) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.23.0->fastsam==0.1.1) (2025.1.31)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.7.0->fastsam==0.1.1) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.7.0->fastsam==0.1.1) (4.12.2)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.7.0->fastsam==0.1.1) (3.4.2)\n",
      "Requirement already satisfied: fsspec in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.7.0->fastsam==0.1.1) (2025.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.7.0->fastsam==0.1.1) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.7.0->fastsam==0.1.1) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.7.0->fastsam==0.1.1) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.7.0->fastsam==0.1.1) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.7.0->fastsam==0.1.1) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.7.0->fastsam==0.1.1) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.7.0->fastsam==0.1.1) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.7.0->fastsam==0.1.1) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.7.0->fastsam==0.1.1) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.7.0->fastsam==0.1.1) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.7.0->fastsam==0.1.1) (11.8.86)\n",
      "Requirement already satisfied: triton==3.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.7.0->fastsam==0.1.1) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.7.0->fastsam==0.1.1) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.7.0->fastsam==0.1.1) (1.3.0)\n",
      "Requirement already satisfied: ftfy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from CLIP@ git+https://github.com/openai/CLIP.git@a1d071733d7111c9c014f024669f959182114e33#egg=CLIP->fastsam==0.1.1) (6.3.1)\n",
      "Requirement already satisfied: regex in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from CLIP@ git+https://github.com/openai/CLIP.git@a1d071733d7111c9c014f024669f959182114e33#egg=CLIP->fastsam==0.1.1) (2024.11.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from altair>=4.2.0->gradio==3.35.2->fastsam==0.1.1) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from altair>=4.2.0->gradio==3.35.2->fastsam==0.1.1) (1.29.0)\n",
      "Requirement already satisfied: anyio in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpx->gradio==3.35.2->fastsam==0.1.1) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpx->gradio==3.35.2->fastsam==0.1.1) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpcore==1.*->httpx->gradio==3.35.2->fastsam==0.1.1) (0.14.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from markdown-it-py>=2.0.0->markdown-it-py[linkify]>=2.0.0->gradio==3.35.2->fastsam==0.1.1) (0.1.2)\n",
      "Requirement already satisfied: linkify-it-py<3,>=1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from markdown-it-py[linkify]>=2.0.0->gradio==3.35.2->fastsam==0.1.1) (2.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->fastsam==0.1.1) (1.17.0)\n",
      "Requirement already satisfied: click>=7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio==3.35.2->fastsam==0.1.1) (8.1.8)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->gradio==3.35.2->fastsam==0.1.1) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->gradio==3.35.2->fastsam==0.1.1) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->gradio==3.35.2->fastsam==0.1.1) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->gradio==3.35.2->fastsam==0.1.1) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->gradio==3.35.2->fastsam==0.1.1) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->gradio==3.35.2->fastsam==0.1.1) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->gradio==3.35.2->fastsam==0.1.1) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->gradio==3.35.2->fastsam==0.1.1) (1.18.3)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastapi->gradio==3.35.2->fastsam==0.1.1) (0.45.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic->gradio==3.35.2->fastsam==0.1.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic->gradio==3.35.2->fastsam==0.1.1) (2.27.2)\n",
      "Requirement already satisfied: wcwidth in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ftfy->CLIP@ git+https://github.com/openai/CLIP.git@a1d071733d7111c9c014f024669f959182114e33#egg=CLIP->fastsam==0.1.1) (0.2.13)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio==3.35.2->fastsam==0.1.1) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio==3.35.2->fastsam==0.1.1) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio==3.35.2->fastsam==0.1.1) (0.23.1)\n",
      "Requirement already satisfied: uc-micro-py in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio==3.35.2->fastsam==0.1.1) (1.0.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio->httpx->gradio==3.35.2->fastsam==0.1.1) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio->httpx->gradio==3.35.2->fastsam==0.1.1) (1.3.1)\n",
      "Requirement already satisfied: ultralytics in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (8.0.0)\n",
      "Requirement already satisfied: hydra-core>=1.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ultralytics) (1.3.2)\n",
      "Requirement already satisfied: matplotlib>=3.2.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ultralytics) (3.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: opencv-python>=4.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ultralytics) (4.11.0.86)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ultralytics) (11.1.0)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ultralytics) (1.11.4)\n",
      "Requirement already satisfied: torch>=1.7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ultralytics) (2.6.0+cu118)\n",
      "Requirement already satisfied: torchvision>=0.8.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ultralytics) (0.21.0+cu118)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: tensorboard>=2.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ultralytics) (2.15.1)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ultralytics) (2.1.4)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ipython in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ultralytics) (8.17.2)\n",
      "Requirement already satisfied: psutil in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ultralytics) (7.0.0)\n",
      "Requirement already satisfied: thop>=0.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ultralytics) (0.1.1.post2209072238)\n",
      "Requirement already satisfied: GitPython>=3.1.24 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ultralytics) (3.1.44)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from GitPython>=3.1.24->ultralytics) (4.0.12)\n",
      "Requirement already satisfied: omegaconf<2.4,>=2.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from hydra-core>=1.2.0->ultralytics) (2.3.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from hydra-core>=1.2.0->ultralytics) (4.9.3)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from hydra-core>=1.2.0->ultralytics) (24.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib>=3.2.2->ultralytics) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib>=3.2.2->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib>=3.2.2->ultralytics) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib>=3.2.2->ultralytics) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib>=3.2.2->ultralytics) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib>=3.2.2->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
      "Requirement already satisfied: absl-py>=0.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard>=2.4.1->ultralytics) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard>=2.4.1->ultralytics) (1.70.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard>=2.4.1->ultralytics) (2.38.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard>=2.4.1->ultralytics) (1.2.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard>=2.4.1->ultralytics) (3.7)\n",
      "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard>=2.4.1->ultralytics) (4.23.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard>=2.4.1->ultralytics) (75.8.2)\n",
      "Requirement already satisfied: six>1.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard>=2.4.1->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard>=2.4.1->ultralytics) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard>=2.4.1->ultralytics) (3.1.3)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics) (2025.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics) (11.8.86)\n",
      "Requirement already satisfied: triton==3.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.7.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: decorator in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython->ultralytics) (5.2.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython->ultralytics) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython->ultralytics) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython->ultralytics) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython->ultralytics) (2.19.1)\n",
      "Requirement already satisfied: stack-data in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython->ultralytics) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython->ultralytics) (5.14.3)\n",
      "Requirement already satisfied: exceptiongroup in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython->ultralytics) (1.2.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython->ultralytics) (4.9.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython>=3.1.24->ultralytics) (5.0.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->ultralytics) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->ultralytics) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->ultralytics) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.4.1->ultralytics) (2.0.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jedi>=0.16->ipython->ultralytics) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pexpect>4.3->ipython->ultralytics) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython->ultralytics) (0.2.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard>=2.4.1->ultralytics) (3.0.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from stack-data->ipython->ultralytics) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from stack-data->ipython->ultralytics) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from stack-data->ipython->ultralytics) (0.2.3)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->ultralytics) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.4.1->ultralytics) (3.2.2)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ultralytics.yolo.cfg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgit+https://github.com/CASIA-IVA-Lab/FastSAM.git\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install ultralytics\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfastsam\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastSAM, FastSAMPrompt\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/fastsam/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Ultralytics YOLO ðŸš€, AGPL-3.0 license\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastSAM\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpredict\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastSAMPredictor\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastSAMPrompt\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/fastsam/model.py:12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Ultralytics YOLO ðŸš€, AGPL-3.0 license\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mFastSAM model interface.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    results = model.predict('ultralytics/assets/bus.jpg')\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01myolo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcfg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_cfg\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01myolo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexporter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Exporter\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01myolo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m YOLO\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ultralytics.yolo.cfg'"
     ]
    }
   ],
   "source": [
    "!pip install 'git+https://github.com/CASIA-IVA-Lab/FastSAM.git'\n",
    "!pip install ultralytics\n",
    "\n",
    "from fastsam import FastSAM, FastSAMPrompt\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def fastsam_segmentation(image_path, device=\"cuda\", model_path=\"FastSAM.pt\"):  # Or \"FastSAM-x.pt\"\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(\"Could not open the image.\")\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Load the FastSAM model\n",
    "    model = FastSAM(model_path)  # Initialize every time\n",
    "    model.to(device)\n",
    "\n",
    "    # Preprocessing (required by FastSAM)\n",
    "    everything_results = model(image, device=device, retina_masks=True, imgsz=1024, conf=0.4, iou=0.9)\n",
    "    # Create a prompt object (this handles the different prompt types)\n",
    "    prompt_process = FastSAMPrompt(image, everything_results, device=device)\n",
    "    # Everything prompt\n",
    "    ann = prompt_process.everything_prompt()\n",
    "    # # Bbox prompt.  Example:  [x1, y1, x2, y2] (top-left, bottom-right)\n",
    "    # ann = prompt_process.box_prompt(bbox=[[200, 200, 400, 400]])\n",
    "    # # Point prompt. Example: points=[[620, 360]], point_label=[1] (1 for foreground, 0 for background)\n",
    "    # ann = prompt_process.point_prompt(points=[[620, 360]], point_label=[1])\n",
    "\n",
    "    if len(ann) == 0:\n",
    "      print(\"No objects detected.\")\n",
    "      # Return a full black image\n",
    "      return np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "\n",
    "    # Find the largest mask (assuming the largest mask is the leaf)\n",
    "    largest_mask = None\n",
    "    largest_area = 0\n",
    "    for mask in ann:\n",
    "      mask_np = mask.cpu().numpy()\n",
    "      area = np.sum(mask_np) # Count the number of True pixels\n",
    "      if area > largest_area:\n",
    "          largest_area = area\n",
    "          largest_mask = mask_np\n",
    "\n",
    "\n",
    "    if largest_mask is not None:\n",
    "         # Convert to uint8 (required for saving as an image)\n",
    "        binary_mask = (largest_mask > 0).astype(np.uint8)\n",
    "    else:\n",
    "      binary_mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "\n",
    "\n",
    "    # # --- Optional: Visualization ---\n",
    "    # plt.imshow(binary_mask, cmap='gray')\n",
    "    # plt.title(\"Segmented Mask\")\n",
    "    # plt.show()\n",
    "\n",
    "    return binary_mask\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Download FastSAM model (if you haven't already)\n",
    "    if not os.path.exists(\"FastSAM.pt\"):\n",
    "        print(\"Downloading FastSAM.pt...\")\n",
    "        os.system(\"wget -q https://github.com/CASIA-IVA-Lab/FastSAM/releases/download/v1.0/FastSAM.pt\") #wget command\n",
    "    image_path = \"/teamspace/studios/this_studio/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Augmented Dataset/Bacterial Blight/augmented_Bacterial Blight0001.jpg\"  # Replace with your image path.\n",
    "    output_mask = fastsam_segmentation(image_path) # Run FastSAM\n",
    "    # Save the mask, or use it for further processing\n",
    "    cv2.imwrite(\"fastsam_output_mask.png\", output_mask * 255)  # Save as grayscale image (0 and 255)\n",
    "    print(\"Segmentation mask saved to fastsam_output_mask.png\")\n",
    "    plt.imshow(output_mask) #Show mask\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderBlock(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): MLPBlock(\n",
      "    (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.0, inplace=False)\n",
      "    (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (4): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.encoder.layers[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Cell 2: Leaf Segmentation (U-Net) - Model and Dataset Definition\n",
    "\n",
    "# --- Simplified U-Net Model ---\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):\n",
    "        super(UNet, self).__init__()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Down part of UNet\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        # Up part of UNet\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    feature*2, feature, kernel_size=2, stride=2,\n",
    "                )\n",
    "            )\n",
    "            self.ups.append(DoubleConv(feature*2, feature))\n",
    "\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
    "\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx+1](concat_skip)\n",
    "\n",
    "        return self.final_conv(x)\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "\n",
    "# --- Dataset ---\n",
    "class CottonLeafSegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        # Modified: List all image files, handling subdirectories\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Iterate through class directories\n",
    "        for class_idx, class_dir in enumerate([item for item in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir,item))]):\n",
    "            class_path = os.path.join(image_dir, class_dir)\n",
    "            for filename in os.listdir(class_path):\n",
    "                if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    self.image_paths.append(os.path.join(class_dir, filename))\n",
    "                    self.labels.append(class_idx)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_filename = self.image_paths[index]\n",
    "        img_path = os.path.join(self.image_dir, img_filename)\n",
    "\n",
    "        # Extract filename without extension\n",
    "        base_filename = os.path.splitext(img_filename)[0]\n",
    "\n",
    "        # Construct the mask path assuming masks are in subdirectories\n",
    "        class_name = [item for item in os.listdir(self.image_dir) if os.path.isdir(os.path.join(self.image_dir,item))][self.labels[index]] #gets class name\n",
    "        mask_path = os.path.join(self.mask_dir, class_name, base_filename + \"_mask.png\")\n",
    "\n",
    "\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32) # L: grayscale\n",
    "        mask[mask > 0.0] = 1.0  # Ensure binary mask (0 and 1)\n",
    "\n",
    "\n",
    "        if self.transform is not None:\n",
    "            augmentations = self.transform(image=image, mask=mask)\n",
    "            image = augmentations[\"image\"]\n",
    "            mask = augmentations[\"mask\"]\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Training Loop ---\n",
    "def train_segmentation_model(image_dir, mask_dir, epochs=25, batch_size=8, learning_rate=1e-4):\n",
    "    # Use albumentations for transforms\n",
    "    transform = A.Compose([\n",
    "        A.Resize(height=256, width=256),\n",
    "        A.Rotate(limit=35, p=0.7),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.1),\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "            max_pixel_value=255.0,\n",
    "        ),\n",
    "        ToTensorV2(), # Convert to tensor\n",
    "    ])\n",
    "    # Modified: Use the updated dataset class\n",
    "    dataset = CottonLeafSegmentationDataset(image_dir, mask_dir, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = UNet(in_channels=3, out_channels=1).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy with Logits (more stable)\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        loop = tqdm(dataloader, total = len(dataloader), leave=False)\n",
    "        for batch_idx, (data, targets) in enumerate(loop):\n",
    "            data = data.to(device=device)\n",
    "            targets = targets.float().unsqueeze(1).to(device=device) # Add channel dimension\n",
    "\n",
    "            # Forward pass\n",
    "            predictions = model(data)\n",
    "            loss = loss_fn(predictions, targets)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update tqdm loop\n",
    "            loop.set_description(f\"Epoch [{epoch}/{epochs}]\")\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"unet_segmentation_model.pth\")  # Save the trained model\n",
    "    return model\n",
    "\n",
    "\n",
    "# --- Inference (using the trained model) ---\n",
    "def segment_leaf(image_path, model, device):\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    transform = A.Compose([  # Inference transform (no augmentation)\n",
    "            A.Resize(height=256, width=256),\n",
    "            A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "            max_pixel_value=255.0,\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    image_np = np.array(image)\n",
    "    augmented = transform(image=image_np)\n",
    "    image_tensor = augmented[\"image\"].unsqueeze(0).to(device) # Add batch dimension\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predicted_mask = model(image_tensor)\n",
    "        predicted_mask = torch.sigmoid(predicted_mask)  # Apply sigmoid for probability\n",
    "        predicted_mask = (predicted_mask > 0.5).float()  # Threshold to get binary mask\n",
    "\n",
    "    return predicted_mask.squeeze().cpu().numpy()  # Remove batch and channel dimensions, move to CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created mask directory and subdirectories: ../SAR-CLD-2024 A Comprehensive Review of Current Research, Challenges, and Future Directions/Original Dataset/masks\n",
      "YOU MUST NOW MANUALLY CREATE SEGMENTATION MASKS.\n",
      "Place the masks inside the corresponding class subfolders within the 'masks' directory.\n",
      "For example, the mask for 'Original Dataset/Bacterial Blight/image1.jpg' should be at:\n",
      "  'Original Dataset/masks/Bacterial Blight/image1_mask.png'\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "Please create masks and then re-run this cell and the following cells.",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Please create masks and then re-run this cell and the following cells.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3556: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# @title Cell 3: U-Net Training and Inference - Example Usage\n",
    "\n",
    "# --- VERY IMPORTANT: Set up paths correctly! ---\n",
    "\n",
    "# 1. Image Directory: Points to your 'Original Dataset' folder.\n",
    "image_directory = \"../SAR-CLD-2024 A Comprehensive Review of Current Research, Challenges, and Future Directions/Original Dataset\"\n",
    "\n",
    "# 2. Mask Directory:  Points to a 'masks' folder *inside* 'Original Dataset'.\n",
    "#    YOU MUST MANUALLY CREATE THIS 'masks' FOLDER AND ITS SUBFOLDERS.\n",
    "mask_directory = \"../SAR-CLD-2024 A Comprehensive Review of Current Research, Challenges, and Future Directions/Original Dataset/masks\"\n",
    "\n",
    "# --- Create the 'masks' directory and its subfolders if they don't exist ---\n",
    "if not os.path.exists(mask_directory):\n",
    "    os.makedirs(mask_directory)\n",
    "    # Create subfolders corresponding to each class in your original dataset\n",
    "    for class_name in os.listdir(image_directory):\n",
    "      class_path = os.path.join(image_directory, class_name)\n",
    "      if os.path.isdir(class_path): # Check to avoid files.\n",
    "          mask_class_path = os.path.join(mask_directory, class_name)\n",
    "          os.makedirs(mask_class_path, exist_ok=True)  # exist_ok=True prevents errors if it already exists\n",
    "    print(f\"Created mask directory and subdirectories: {mask_directory}\")\n",
    "    print(\"YOU MUST NOW MANUALLY CREATE SEGMENTATION MASKS.\")\n",
    "    print(\"Place the masks inside the corresponding class subfolders within the 'masks' directory.\")\n",
    "    print(\"For example, the mask for 'Original Dataset/Bacterial Blight/image1.jpg' should be at:\")\n",
    "    print(\"  'Original Dataset/masks/Bacterial Blight/image1_mask.png'\")\n",
    "    # Exit early if the mask directory was just created.  This prevents errors.\n",
    "    #  The user needs to create the masks before proceeding.\n",
    "    import sys\n",
    "    sys.exit(\"Please create masks and then re-run this cell and the following cells.\")\n",
    "\n",
    "\n",
    "# --- Create dummy images (ONLY for demonstration if the directory is empty) ---\n",
    "# In a REAL scenario, you would already have your images and you would create\n",
    "# the masks manually.  This section is just to make the notebook runnable\n",
    "# for demonstration purposes.\n",
    "for class_name in os.listdir(image_directory):\n",
    "    class_path = os.path.join(image_directory, class_name)\n",
    "    if os.path.isdir(class_path) and not os.listdir(class_path): #check if it is a directory and if it is empty\n",
    "        print(f\"Creating dummy images in: {class_path}\")\n",
    "        for i in range(5):  # Create a few dummy images\n",
    "            img = Image.new('RGB', (256, 256))  # Create a blank image\n",
    "            img.save(os.path.join(class_path, f\"image_{i}.jpg\"))\n",
    "\n",
    "\n",
    "# --- Train the U-Net Model (if masks exist) ---\n",
    "if os.path.exists(mask_directory) and any(os.scandir(mask_directory)): # Check if masks dir exists and is not empty\n",
    "    print(\"Training U-Net model...\")\n",
    "    trained_model = train_segmentation_model(image_directory, mask_directory)\n",
    "\n",
    "    # --- Example Inference ---\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    trained_model.to(device)\n",
    "\n",
    "    # Get an example image path (you'll need to adapt this if your images are named differently)\n",
    "    example_class = os.listdir(image_directory)[0]  # Get the first class directory\n",
    "    example_image_path = os.path.join(image_directory, example_class, os.listdir(os.path.join(image_directory,example_class))[0])\n",
    "    if os.path.exists(example_image_path):\n",
    "        segmented_mask = segment_leaf(example_image_path, trained_model, device)\n",
    "\n",
    "\n",
    "        # Display the result\n",
    "        original_image = Image.open(example_image_path)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(original_image)\n",
    "        plt.title(\"Original Image\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(segmented_mask, cmap='gray')\n",
    "        plt.title(\"Segmented Mask\")\n",
    "        plt.show()\n",
    "\n",
    "        # --- Applying the mask ---\n",
    "        # 1. Masking (setting background to black)\n",
    "        original_image_np = np.array(original_image)\n",
    "        segmented_mask_expanded = np.expand_dims(segmented_mask, axis=2)\n",
    "        masked_image = original_image_np * segmented_mask_expanded\n",
    "        masked_image = masked_image.astype(np.uint8)\n",
    "\n",
    "        # 2. Cropping (finding bounding box)\n",
    "        coords = np.argwhere(segmented_mask > 0)\n",
    "        if coords.size > 0:\n",
    "            y_min, x_min = coords.min(axis=0)\n",
    "            y_max, x_max = coords.max(axis=0)\n",
    "            cropped_image = original_image_np[y_min:y_max+1, x_min:x_max+1]\n",
    "            cropped_image = cropped_image.astype(np.uint8)\n",
    "\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.imshow(masked_image)\n",
    "            plt.title(\"Masked Image\")\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.imshow(cropped_image)\n",
    "            plt.title(\"Cropped Image\")\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"No leaf detected in the image.\")\n",
    "    else:\n",
    "      print(\"Example Image not found.\")\n",
    "else:\n",
    "    print(\"Mask directory is empty or does not exist.  Skipping U-Net training and inference.\")\n",
    "    print(\"Please create segmentation masks and place them in the 'masks' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Cell 4: Noise Reduction\n",
    "\n",
    "def apply_noise_reduction(image_path):\n",
    "    \"\"\"Applies and visualizes different noise reduction techniques.\"\"\"\n",
    "\n",
    "    original_image = cv2.imread(image_path)\n",
    "    if original_image is None:\n",
    "        raise FileNotFoundError(f\"Could not open or find the image at {image_path}\")\n",
    "\n",
    "    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "    image = img_as_float(original_image) # Convert to float for skimage functions\n",
    "\n",
    "    # --- Non-Local Means Denoising ---\n",
    "    denoised_nl_means = denoise_nl_means(image, patch_size=7, patch_distance=11, h=0.1, multichannel=True)\n",
    "\n",
    "    # --- Bilateral Filtering ---\n",
    "    denoised_bilateral = denoise_bilateral(image, sigma_color=0.05, sigma_spatial=15, multichannel=True)\n",
    "\n",
    "    # --- Wavelet Denoising ---\n",
    "    denoised_wavelet = denoise_wavelet(image, multichannel=True, convert2ycbcr=True, method='BayesShrink')\n",
    "\n",
    "    # --- OpenCV FastNlMeansDenoisingColored (for comparison) ---\n",
    "    denoised_cv2 = cv2.fastNlMeansDenoisingColored(original_image, None, 10, 10, 7, 21)\n",
    "    denoised_cv2 = cv2.cvtColor(denoised_cv2, cv2.COLOR_BGR2RGB) # Convert to RGB\n",
    "\n",
    "\n",
    "    # --- Visualization ---\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(20, 5))\n",
    "\n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title('Original')\n",
    "\n",
    "    axes[1].imshow(img_as_ubyte(denoised_nl_means)) # Convert back to uint8 for display\n",
    "    axes[1].set_title('NL Means (skimage)')\n",
    "\n",
    "    axes[2].imshow(img_as_ubyte(denoised_bilateral))\n",
    "    axes[2].set_title('Bilateral (skimage)')\n",
    "\n",
    "    axes[3].imshow(img_as_ubyte(denoised_wavelet))\n",
    "    axes[3].set_title('Wavelet (skimage)')\n",
    "\n",
    "    axes[4].imshow(denoised_cv2)  # Already uint8\n",
    "    axes[4].set_title('NL Means (OpenCV)')\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return original_image, img_as_ubyte(denoised_nl_means), img_as_ubyte(denoised_bilateral), img_as_ubyte(denoised_wavelet), denoised_cv2\n",
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "\n",
    "# Create a dummy noisy image (for demonstration)\n",
    "image_path = \"noisy_image.jpg\"\n",
    "if not os.path.exists(image_path):\n",
    "  img = np.zeros((256, 256, 3), dtype=np.uint8)\n",
    "  img[:] = (50, 100, 150)  # Fill with a solid color\n",
    "  noise = np.random.randint(0, 50, img.shape, dtype=np.uint8)  # Add random noise\n",
    "  noisy_img = cv2.add(img, noise)\n",
    "  cv2.imwrite(image_path, cv2.cvtColor(noisy_img, cv2.COLOR_RGB2BGR))  # Save as BGR for OpenCV\n",
    "\n",
    "\n",
    "original, nl_means, bilateral, wavelet, cv2_nl_means = apply_noise_reduction(image_path)\n",
    "\n",
    "# Example of integrating into a PyTorch data pipeline:\n",
    "# Convert the denoised image (e.g., nl_means) to a PIL Image\n",
    "# denoised_pil = Image.fromarray(nl_means)\n",
    "# Then, you can use torchvision.transforms to convert it to a tensor:\n",
    "# transform = transforms.Compose([transforms.ToTensor()])\n",
    "# denoised_tensor = transform(denoised_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Cell 5: Leaf Orientation Normalization\n",
    "\n",
    "def normalize_leaf_orientation(image_path, segmented_mask=None):\n",
    "    \"\"\"Normalizes leaf orientation using PCA on the segmentation mask.\"\"\"\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"Could not open or find image at {image_path}\")\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    if segmented_mask is None:\n",
    "        # If no mask provided, do simple thresholding (fallback)\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        _, thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\n",
    "        mask = thresh.astype(np.uint8)\n",
    "    else:\n",
    "        mask = (segmented_mask * 255).astype(np.uint8)  # Ensure 0-255 range\n",
    "\n",
    "    coords = np.argwhere(mask > 0)\n",
    "    if coords.size == 0:\n",
    "        print(\"Warning: No leaf pixels found. Returning original image.\")\n",
    "        return image, 0, mask\n",
    "\n",
    "    # PCA\n",
    "    mean = coords.mean(axis=0)\n",
    "    centered_coords = coords - mean\n",
    "    covariance_matrix = np.cov(centered_coords, rowvar=False)\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n",
    "\n",
    "    principal_eigenvector = eigenvectors[:, 0]\n",
    "    angle_rad = np.arctan2(principal_eigenvector[0], principal_eigenvector[1])\n",
    "    angle_deg = np.degrees(angle_rad)\n",
    "\n",
    "    # Rotate\n",
    "    rotated_image = rotate(image, angle_deg, resize=True, preserve_range=True)\n",
    "    rotated_mask = rotate(mask, angle_deg, resize=True, preserve_range=True, order=0)\n",
    "\n",
    "    rotated_image = img_as_ubyte(rotated_image)\n",
    "    rotated_mask = (rotated_mask > 0.5).astype(np.uint8)\n",
    "\n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[1].imshow(mask, cmap='gray')\n",
    "    axes[1].set_title('Segmentation Mask')\n",
    "    center_y, center_x = mean\n",
    "    axes[1].arrow(center_x, center_y, 50 * np.cos(angle_rad), 50*np.sin(angle_rad),\n",
    "                 color='red', width=2, head_width=8)\n",
    "    axes[2].imshow(rotated_image)\n",
    "    axes[2].set_title(f'Rotated Image ({angle_deg:.2f}Â°)')\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return rotated_image, angle_deg, rotated_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Cell 6: Orientation Normalization - Example Usage\n",
    "\n",
    "# --- Dummy Image and Mask Creation (for demonstration) ---\n",
    "image_path = \"orientation_image.jpg\"\n",
    "mask_path = \"orientation_mask.png\"\n",
    "\n",
    "if not os.path.exists(image_path) or not os.path.exists(mask_path):\n",
    "    img = np.zeros((256, 256, 3), dtype=np.uint8)\n",
    "    for i in range(256):\n",
    "        img[i, i] = (255, 255, 255)  # White diagonal line\n",
    "    cv2.imwrite(image_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
    "    mask = np.zeros((256, 256), dtype=np.uint8)\n",
    "    for i in range(256):\n",
    "        mask[i, i] = 255\n",
    "    cv2.imwrite(mask_path, mask)\n",
    "\n",
    "# --- Example without a provided mask (using simple thresholding) ---\n",
    "rotated_image, angle, _ = normalize_leaf_orientation(image_path)\n",
    "print(f\"Detected angle (no mask provided): {angle:.2f} degrees\")\n",
    "\n",
    "# --- Example *with* a mask (using U-Net output) ---\n",
    "# This assumes you've run the U-Net training and inference (Cells 2 and 3).\n",
    "if os.path.exists(\"unet_segmentation_model.pth\"):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    seg_model = UNet(in_channels=3, out_channels=1)  # Instantiate U-Net\n",
    "    seg_model.load_state_dict(torch.load(\"unet_segmentation_model.pth\"))\n",
    "    seg_model.to(device)\n",
    "\n",
    "    #  Use a *real* image from your dataset for a proper test.\n",
    "    #  This assumes your 'Original Dataset' has at least one image.\n",
    "    if os.path.exists(image_directory) and any(os.scandir(image_directory)):\n",
    "        example_class_dir = os.listdir(image_directory)[0]\n",
    "        example_image_name = os.listdir(os.path.join(image_directory,example_class_dir))[0]\n",
    "        example_image_path = os.path.join(image_directory, example_class_dir, example_image_name ) #Use the image that used in segmentation\n",
    "\n",
    "        segmented_mask_example = segment_leaf(example_image_path, seg_model, device)\n",
    "        rotated_image_with_mask, angle_with_mask, rotated_mask = normalize_leaf_orientation(example_image_path, segmented_mask_example)\n",
    "        print(f\"Detected angle (with mask provided): {angle_with_mask:.2f} degrees\")\n",
    "\n",
    "         # Example of integrating into PyTorch dataset:\n",
    "        if rotated_mask is not None:\n",
    "            # Convert rotated image and mask to PIL Images:\n",
    "            rotated_image_pil = Image.fromarray(rotated_image)\n",
    "            rotated_mask_pil = Image.fromarray((rotated_mask * 255).astype(np.uint8))\n",
    "\n",
    "            # Now use torchvision.transforms:\n",
    "            transform = A.Compose([\n",
    "                    A.Resize(height=256, width=256),\n",
    "                    A.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225],\n",
    "                    max_pixel_value=255.0,\n",
    "                    ),\n",
    "                    ToTensorV2(),\n",
    "                ])\n",
    "            augmented = transform(image=np.array(rotated_image_pil), mask=np.array(rotated_mask_pil))\n",
    "            rotated_image_tensor = augmented['image']\n",
    "            rotated_mask_tensor = augmented['mask']\n",
    "    else:\n",
    "        print(\"Skipping mask example: 'Original Dataset' is empty or does not exist.\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping mask example: 'unet_segmentation_model.pth' not found. Run U-Net example first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Cell 7: GAN-Based Synthetic Data Generation (cGAN)\n",
    "\n",
    "# --- Simplified cGAN Generator ---\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, img_channels, num_classes, features_g, img_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.embed = nn.Embedding(num_classes, img_size * img_size)\n",
    "        self.net = nn.Sequential(\n",
    "            # Input: N x (z_dim + num_classes) x 1 x 1  (noise + embedded label)\n",
    "            self._block(z_dim + img_size*img_size, features_g * 16, 4, 1, 0),  # N x f_g*16 x 4 x 4\n",
    "            self._block(features_g * 16, features_g * 8, 4, 2, 1),  # N x f_g*8 x 8 x 8\n",
    "            self._block(features_g * 8, features_g * 4, 4, 2, 1),  # N x f_g*4 x 16 x 16\n",
    "            self._block(features_g * 4, features_g * 2, 4, 2, 1),  # N x f_g*2 x 32 x 32\n",
    "            nn.ConvTranspose2d(\n",
    "                features_g * 2, img_channels, kernel_size=4, stride=2, padding=1\n",
    "            ),  # N x img_channels x 64 x 64\n",
    "            nn.Tanh(), # [-1, 1] range\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "      # Latent vector x: [N, z_dim, 1, 1]\n",
    "      # labels: [N]\n",
    "      embedding = self.embed(labels).view(labels.shape[0], self.img_size, self.img_size)\n",
    "      x = torch.cat([x, embedding], dim=1)  # Concatenate noise with embedded label\n",
    "      return self.net(x)\n",
    "\n",
    "\n",
    "# --- Simplified cGAN Discriminator ---\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channels, num_classes, features_d, img_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.embed = nn.Embedding(num_classes, img_size * img_size)\n",
    "        self.net = nn.Sequential(\n",
    "            # Input: N x (img_channels + num_classes) x 64 x 64\n",
    "            nn.Conv2d(img_channels + 1, features_d, kernel_size=4, stride=2, padding=1), # +1 for label\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # _block(in_channels, out_channels, kernel_size, stride, padding)\n",
    "            self._block(features_d, features_d * 2, 4, 2, 1),\n",
    "            self._block(features_d * 2, features_d * 4, 4, 2, 1),\n",
    "            self._block(features_d * 4, features_d * 8, 4, 2, 1),\n",
    "            # After all _block img output is 4x4 (Conv2d below makes into 1x1)\n",
    "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n",
    "            nn.Sigmoid(),  # Output a single probability (real/fake)\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        # x: [N, img_channels, img_size, img_size]\n",
    "        # labels: [N]\n",
    "        embedding = self.embed(labels).view(labels.shape[0], 1, self.img_size, self.img_size)\n",
    "        x = torch.cat([x, embedding], dim=1) # Concatenate image with embedded label\n",
    "        return self.net(x)\n",
    "\n",
    "# --- Dataset ---\n",
    "class CottonLeafDiseaseDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, num_classes = 4): #Assuming there is 4 disease classes\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.num_classes = num_classes\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Iterate through class directories (assuming each disease has its own folder)\n",
    "        for class_idx in range(num_classes):\n",
    "          class_dir = os.path.join(root_dir, str(class_idx))\n",
    "          if not os.path.isdir(class_dir):\n",
    "              #Skip files.\n",
    "              continue\n",
    "          for filename in os.listdir(class_dir):\n",
    "              if filename.endswith(('.jpg', '.jpeg', '.png')):  # Add other extensions if needed\n",
    "                  self.image_paths.append(os.path.join(class_dir, filename))\n",
    "                  self.labels.append(class_idx)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image=np.array(image))['image']\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# --- Training Loop ---\n",
    "def train_cgan(root_dir, epochs=100, batch_size=64, z_dim=100, lr=2e-4, img_size = 64, num_classes=4):\n",
    "    # Transforms\n",
    "    transform = A.Compose(\n",
    "        [\n",
    "            A.Resize(height=img_size, width=img_size),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.Normalize(\n",
    "                mean=[0.5, 0.5, 0.5],\n",
    "                std=[0.5, 0.5, 0.5],\n",
    "                max_pixel_value=255,\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    dataset = CottonLeafDiseaseDataset(root_dir, transform=transform, num_classes=num_classes)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4) # Added num_workers\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize generator and discriminator\n",
    "    netG = Generator(z_dim, 3, num_classes, 64, img_size).to(device)  # 3 for RGB channels\n",
    "    netD = Discriminator(3, num_classes, 64, img_size).to(device)\n",
    "\n",
    "    # Optimizers\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "    # Loss function\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Create a directory for saving generated images\n",
    "    output_dir = \"synthetic_images\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(\"Starting Training...\")\n",
    "    for epoch in range(epochs):\n",
    "        netG.train()\n",
    "        netD.train()\n",
    "        loop = tqdm(dataloader, total=len(dataloader), leave=False)\n",
    "        for batch_idx, (real_images, labels) in enumerate(loop):\n",
    "            real_images = real_images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            batch_size_current = real_images.size(0) # Get actual batch size (last batch might be smaller)\n",
    "\n",
    "            # --- Train Discriminator: max log(D(x)) + log(1 - D(G(z))) ---\n",
    "            netD.zero_grad()\n",
    "            # 1. Train with real images\n",
    "            real_labels = torch.ones(batch_size_current, 1, 1, 1).to(device) # Real label: 1\n",
    "            output_real = netD(real_images, labels).view(-1, 1, 1, 1) # Get the prediction\n",
    "            lossD_real = criterion(output_real, real_labels)\n",
    "            lossD_real.backward()\n",
    "\n",
    "\n",
    "            # 2. Train with fake images\n",
    "            noise = torch.randn(batch_size_current, z_dim, 1, 1).to(device)\n",
    "            fake_labels = torch.randint(0, num_classes, (batch_size_current,)).to(device)  # Random class labels\n",
    "            fake_images = netG(noise, fake_labels)\n",
    "            fake_labels_tensor = torch.ones(batch_size_current, 1, 1, 1).to(device) * 0  # Fake label: 0\n",
    "            output_fake = netD(fake_images.detach(), fake_labels).view(-1, 1, 1, 1) # Detach to avoid training G\n",
    "            lossD_fake = criterion(output_fake, fake_labels_tensor)\n",
    "            lossD_fake.backward()\n",
    "\n",
    "            lossD = lossD_real + lossD_fake\n",
    "            optimizerD.step()\n",
    "\n",
    "            # --- Train Generator: min log(1 - D(G(z)))  <-> max log(D(G(z)) ---\n",
    "            netG.zero_grad()\n",
    "            output = netD(fake_images, fake_labels).view(-1, 1, 1, 1)\n",
    "            lossG = criterion(output, real_labels) # We want G to fool D (predict 1)\n",
    "            lossG.backward()\n",
    "            optimizerG.step()\n",
    "\n",
    "\n",
    "            # update tqdm loop\n",
    "            loop.set_description(f\"Epoch [{epoch}/{epochs}]\")\n",
    "            loop.set_postfix(lossD=lossD.item(), lossG = lossG.item())\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss D: {lossD.item():.4f}, Loss G: {lossG.item():.4f}\")\n",
    "\n",
    "\n",
    "        # --- Generate and Save Synthetic Images (after each epoch) ---\n",
    "        netG.eval()  # Set generator to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for class_idx in range(num_classes):\n",
    "                fixed_noise = torch.randn(16, z_dim, 1, 1).to(device)  # Generate 16 images per class\n",
    "                fixed_labels = torch.full((16,), class_idx, dtype=torch.long).to(device)\n",
    "                fake_images = netG(fixed_noise, fixed_labels)\n",
    "\n",
    "                # Denormalize images (from [-1, 1] to [0, 1])\n",
    "                fake_images = fake_images * 0.5 + 0.5\n",
    "\n",
    "                # Save images\n",
    "                for i in range(16):\n",
    "                    img = fake_images[i].cpu().permute(1, 2, 0).numpy()  # CHW to HWC\n",
    "                    img = (img * 255).astype(np.uint8) # Scale to 0-255\n",
    "                    img_pil = Image.fromarray(img)\n",
    "                    img_pil.save(os.path.join(output_dir, f\"epoch_{epoch}_class_{class_idx}_img_{i}.png\"))\n",
    "    return netG, netD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Cell 8: GAN Training and Image Generation\n",
    "\n",
    "# --- Create a dummy dataset structure (for demonstration) ---\n",
    "root_directory = \"../SAR-CLD-2024 A Comprehensive Review of Current Research, Challenges, and Future Directions/Original Dataset\"\n",
    "num_classes = 4  # Example: 4 disease classes\n",
    "img_size = 64\n",
    "\n",
    "# Check if the directory exists and contains subdirectories for each class.\n",
    "# If not, create dummy data.\n",
    "if not os.path.exists(root_directory) or not any(os.scandir(root_directory)):\n",
    "    os.makedirs(root_directory, exist_ok=True)\n",
    "    for i in range(num_classes):\n",
    "        class_dir = os.path.join(root_directory, str(i))\n",
    "        os.makedirs(class_dir, exist_ok=True)\n",
    "        for j in range(20):\n",
    "            img = np.random.randint(0, 256, size=(img_size, img_size, 3), dtype=np.uint8)\n",
    "            img_pil = Image.fromarray(img)\n",
    "            img_pil.save(os.path.join(class_dir, f\"img_{j}.jpg\"))\n",
    "    print(f\"Created dummy dataset at: {root_directory}\")\n",
    "\n",
    "\n",
    "# --- Train the cGAN ---\n",
    "# Use a smaller number of epochs for demonstration.  Increase for real training.\n",
    "trained_generator, trained_discriminator = train_cgan(root_directory, epochs=5, num_classes=num_classes)\n",
    "\n",
    "\n",
    "# --- Example of using the trained generator ---\n",
    "def generate_synthetic_images(generator, z_dim, num_classes, num_samples_per_class, device):\n",
    "    generator.eval()\n",
    "    synthetic_images = []\n",
    "    synthetic_labels = []\n",
    "    with torch.no_grad():\n",
    "        for class_idx in range(num_classes):\n",
    "            noise = torch.randn(num_samples_per_class, z_dim, 1, 1).to(device)\n",
    "            labels = torch.full((num_samples_per_class,), class_idx, dtype=torch.long).to(device)\n",
    "            fake_images = generator(noise, labels)\n",
    "            fake_images = fake_images * 0.5 + 0.5  # Denormalize\n",
    "            synthetic_images.append(fake_images)\n",
    "            synthetic_labels.extend([class_idx] * num_samples_per_class)\n",
    "    return torch.cat(synthetic_images, dim=0), synthetic_labels\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "z_dim = 100\n",
    "num_samples_per_class = 10\n",
    "synthetic_images, synthetic_labels = generate_synthetic_images(trained_generator, z_dim, num_classes, num_samples_per_class, device)\n",
    "\n",
    "# --- Visualize Synthetic Images ---\n",
    "fig, axes = plt.subplots(nrows=num_classes, ncols=num_samples_per_class, figsize=(12, 6))\n",
    "for i in range(len(synthetic_images)):\n",
    "    img = synthetic_images[i].cpu().permute(1, 2, 0).numpy()\n",
    "    img = (img * 255).astype(np.uint8)\n",
    "    row = i // num_samples_per_class\n",
    "    col = i % num_samples_per_class\n",
    "    axes[row, col].imshow(img)\n",
    "    axes[row, col].axis('off')\n",
    "    if col == 0:\n",
    "        axes[row, col].set_ylabel(f\"Class {synthetic_labels[i]}\", rotation=0, labelpad=30)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Cell 9: Datasets for Real and Synthetic Data\n",
    "\n",
    "# --- Dataset for REAL Cotton Leaf Images (for training and validation) ---\n",
    "class RealCottonLeafDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, train=True, validation_split=0.2):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Assuming same directory structure as before (class folders)\n",
    "        for class_idx in range(4):  # Adjust num_classes if needed.\n",
    "            class_dir = os.path.join(root_dir, str(class_idx))\n",
    "            if not os.path.isdir(class_dir):\n",
    "                continue  # Skip if the class directory doesn't exist\n",
    "            for filename in os.listdir(class_dir):\n",
    "                if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    self.image_paths.append(os.path.join(class_dir, filename))\n",
    "                    self.labels.append(class_idx)\n",
    "\n",
    "        # Split into training and validation sets\n",
    "        train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "            self.image_paths, self.labels, test_size=validation_split, random_state=42, stratify=self.labels\n",
    "        )\n",
    "\n",
    "        if self.train:\n",
    "            self.image_paths = train_paths\n",
    "            self.labels = train_labels\n",
    "        else:\n",
    "            self.image_paths = val_paths\n",
    "            self.labels = val_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "           image = self.transform(image=np.array(image))['image']\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# --- Dataset for SYNTHETIC Cotton Leaf Images ---\n",
    "class SyntheticCottonLeafDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Assuming images are saved as \"epoch_{epoch}_class_{class_idx}_img_{i}.png\"\n",
    "        for filename in os.listdir(self.root_dir):\n",
    "            if filename.endswith(\".png\") and filename.startswith(\"epoch_\"):\n",
    "                parts = filename.split(\"_\")\n",
    "                try:\n",
    "                    class_idx = int(parts[3])  # Extract class index\n",
    "                    self.image_paths.append(os.path.join(self.root_dir, filename))\n",
    "                    self.labels.append(class_idx)\n",
    "                except (ValueError, IndexError):\n",
    "                    print(f\"Warning: Skipping malformed filename: {filename}\")\n",
    "                    continue\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image=np.array(image))['image']\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Cell 10: Combined Training Function\n",
    "\n",
    "# --- Combined Training Function (Example with a simple CNN) ---\n",
    "def train_combined(real_data_dir, synthetic_data_dir, epochs=20, batch_size=32, learning_rate=1e-4, synthetic_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Trains a model using a combination of real and synthetic data.\n",
    "\n",
    "    Args:\n",
    "        real_data_dir: Path to the directory containing real cotton leaf images.\n",
    "        synthetic_data_dir: Path to the directory containing synthetic images.\n",
    "        epochs: Number of training epochs.\n",
    "        batch_size: Batch size.\n",
    "        learning_rate: Learning rate.\n",
    "        synthetic_ratio:  The proportion of synthetic data in each batch (0.0 to 1.0).\n",
    "    \"\"\"\n",
    "\n",
    "    # Transforms (adjust as needed for your main model)\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(height=224, width=224), # Example size\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.1),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    val_transform = A.Compose([ # Usually less augmentation for validation\n",
    "        A.Resize(height=224, width=224),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "\n",
    "    # Create datasets\n",
    "    real_train_dataset = RealCottonLeafDataset(real_data_dir, transform=train_transform, train=True)\n",
    "    real_val_dataset = RealCottonLeafDataset(real_data_dir, transform=val_transform, train=False)  # Validation set (only real data)\n",
    "    synthetic_train_dataset = SyntheticCottonLeafDataset(synthetic_data_dir, transform=train_transform)\n",
    "\n",
    "    # Create a combined dataset using ConcatDataset\n",
    "    combined_dataset = ConcatDataset([real_train_dataset, synthetic_train_dataset])\n",
    "\n",
    "    # Create DataLoaders.  We'll use a custom sampler to control the synthetic ratio.\n",
    "    #  The built-in sampler doesn't guarantee a specific ratio *within each batch*.\n",
    "\n",
    "    class MixedBatchSampler(torch.utils.data.Sampler):\n",
    "        def __init__(self, real_dataset, synthetic_dataset, batch_size, synthetic_ratio):\n",
    "            self.real_indices = list(range(len(real_dataset)))\n",
    "            self.synthetic_indices = list(range(len(real_dataset), len(real_dataset) + len(synthetic_dataset)))\n",
    "            self.batch_size = batch_size\n",
    "            self.synthetic_ratio = synthetic_ratio\n",
    "            self.num_synthetic = int(batch_size * synthetic_ratio)\n",
    "            self.num_real = batch_size - self.num_synthetic\n",
    "            self.length = len(real_dataset) // self.num_real  # Determine how many batches we can create\n",
    "\n",
    "        def __iter__(self):\n",
    "            np.random.shuffle(self.real_indices)\n",
    "            np.random.shuffle(self.synthetic_indices)\n",
    "            real_iter = iter(self.real_indices)\n",
    "            synth_iter = iter(self.synthetic_indices)\n",
    "\n",
    "            for _ in range(self.length):\n",
    "                batch = []\n",
    "                # Get real samples for this batch\n",
    "                for _ in range(self.num_real):\n",
    "                    try:\n",
    "                        batch.append(next(real_iter))\n",
    "                    except StopIteration:  # Handle edge case if real data runs out\n",
    "                        real_iter = iter(self.real_indices)\n",
    "                        batch.append(next(real_iter))\n",
    "                # Get synthetic samples for this batch\n",
    "                for _ in range(self.num_synthetic):\n",
    "                    try:\n",
    "                        batch.append(next(synth_iter))\n",
    "                    except StopIteration:  # Handle edge case\n",
    "                        synth_iter = iter(self.synthetic_indices)\n",
    "                        batch.append(next(synth_iter))\n",
    "                np.random.shuffle(batch)  # Shuffle the combined batch\n",
    "                yield batch\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.length # Number of *batches*\n",
    "\n",
    "\n",
    "    sampler = MixedBatchSampler(real_train_dataset, synthetic_train_dataset, batch_size, synthetic_ratio)\n",
    "    train_loader = DataLoader(combined_dataset, batch_sampler=sampler, num_workers=4)\n",
    "    val_loader = DataLoader(real_val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "    # --- Model (Replace with your ViT or Ensemble) ---\n",
    "    # For demonstration, use a simple CNN.  *You should replace this with your actual model.*\n",
    "    model = torch.hub.load('pytorch/vision:v0.9.0', 'resnet18', pretrained=True)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 4)  # 4 classes (adjust as needed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss() #Use cross entropy for multi class classification\n",
    "\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        loop = tqdm(train_loader, total=len(train_loader), leave=False)\n",
    "        for batch_idx, (inputs, labels) in enumerate(loop):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "            loop.set_description(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "            loop.set_postfix(loss=loss.item(), acc=(100 * correct_train / total_train))\n",
    "\n",
    "\n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "        # --- Validation (on REAL data) ---\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "        val_accuracy = 100 * correct_val / total_val\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Val Loss: {val_loss/len(val_loader):.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Cell 11: Example Usage of Combined Training\n",
    "\n",
    "# --- Set up paths ---\n",
    "real_data_root = \"../SAR-CLD-2024 A Comprehensive Review of Current Research, Challenges, and Future Directions/Original Dataset\"\n",
    "synthetic_data_root = \"synthetic_images\"  # Should have been created by the GAN\n",
    "\n",
    "# --- Create Dummy Real Data (if needed) ---\n",
    "# This is for demonstration purposes.  In a real scenario, you would already have your real data.\n",
    "num_classes = 4\n",
    "img_size = 224\n",
    "if not os.path.exists(real_data_root) or not any(os.scandir(os.path.join(real_data_root, str(0)))): #Check if the path or the subfolders exists\n",
    "    os.makedirs(real_data_root, exist_ok=True)\n",
    "    for i in range(num_classes):\n",
    "        class_dir = os.path.join(real_data_root, str(i))\n",
    "        os.makedirs(class_dir, exist_ok=True)\n",
    "        for j in range(30):  # Create more real images than synthetic\n",
    "            img = np.random.randint(0, 256, size=(img_size, img_size, 3), dtype=np.uint8)\n",
    "            img_pil = Image.fromarray(img)\n",
    "            img_pil.save(os.path.join(class_dir, f\"real_img_{j}.jpg\"))\n",
    "    print(f\"Created dummy REAL dataset at: {real_data_root}\")\n",
    "\n",
    "# --- Check if synthetic data exists ---\n",
    "if not os.path.exists(synthetic_data_root):\n",
    "    print(\"ERROR: Synthetic data directory not found. Run GAN training first.\")\n",
    "else:\n",
    "    # --- Train the combined model ---\n",
    "    trained_model = train_combined(real_data_root, synthetic_data_root, epochs=10, synthetic_ratio=0.3)\n",
    "\n",
    "    # --- Save the trained model ---\n",
    "    torch.save(trained_model.state_dict(), \"combined_model.pth\")\n",
    "    print(\"Trained model saved to 'combined_model.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 4.3 Example Usage and Model Saving (Cell 12)\n",
    "# Create Dummy Data (replace with your actual paths)\n",
    "real_data_root = \"dummy_real_dataset\"\n",
    "synthetic_data_root = \"synthetic_images\"  # From the GAN training output\n",
    "\n",
    "# Create dummy real data (similar to the GAN dummy data)\n",
    "if not os.path.exists(real_data_root):\n",
    "    os.makedirs(real_data_root)\n",
    "    num_classes = 4\n",
    "    img_size = 224 #Should match with the training configuration\n",
    "    for i in range(num_classes):\n",
    "        class_dir = os.path.join(real_data_root, str(i))\n",
    "        os.makedirs(class_dir)\n",
    "        for j in range(30): # More images per class for real data\n",
    "            img = np.random.randint(0, 256, size=(img_size, img_size, 3), dtype=np.uint8)\n",
    "            img_pil = Image.fromarray(img)\n",
    "            img_pil.save(os.path.join(class_dir, f\"real_img_{j}.jpg\"))\n",
    "\n",
    "    print(f\"Created dummy REAL dataset at: {real_data_root}\")\n",
    "\n",
    "# Check if synthetic data exists (assuming you ran the GAN training)\n",
    "if not os.path.exists(synthetic_data_root):\n",
    "    print(\"ERROR: Synthetic data directory not found.  Run the GAN training code first.\")\n",
    "else:\n",
    "    # Train the combined model\n",
    "    trained_model = train_combined(real_data_root, synthetic_data_root, epochs=10, synthetic_ratio=0.3)  # Example: 30% synthetic data\n",
    "\n",
    "    # After training, you can save and evaluate your model as usual.\n",
    "    torch.save(trained_model.state_dict(), \"combined_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Cell 12 (Optional): Loading and Evaluating the Combined Model\n",
    "\n",
    "# --- Example of loading and evaluating the model ---\n",
    "# This is optional and needs to be adapted to your specific evaluation setup.\n",
    "# It assumes you have a separate *test set* of real images.\n",
    "\n",
    "def evaluate_model(model_path, test_data_dir):\n",
    "    \"\"\"Loads a saved model and evaluates it on a test dataset.\"\"\"\n",
    "\n",
    "    # --- Model (must match the architecture used during training) ---\n",
    "    model = torch.hub.load('pytorch/vision:v0.9.0', 'resnet18', pretrained=True)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 4)  # 4 classes\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # --- Load the saved model weights ---\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()  # Set to evaluation mode\n",
    "\n",
    "    # --- Transforms for the test set (usually no augmentation) ---\n",
    "    test_transform = A.Compose([\n",
    "        A.Resize(height=224, width=224),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    # --- Create a test dataset (assuming same directory structure as training) ---\n",
    "    test_dataset = RealCottonLeafDataset(test_data_dir, transform=test_transform, train=False) # Use Real dataset and set train to False to get all\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # --- Evaluation Loop ---\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_predictions.extend(predicted.cpu().numpy())  # For confusion matrix\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Accuracy on the test set: {accuracy:.2f}%\")\n",
    "\n",
    "    # --- Optional: Confusion Matrix ---\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    import seaborn as sns\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=range(4), yticklabels=range(4)) #Modify xticklabels, yticklabels based on dataset\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    # --- Optional: Classification Report ---\n",
    "    print(classification_report(all_labels, all_predictions))\n",
    "# --- Example Usage (uncomment and modify paths if you have a test set) ---\n",
    "# test_data_directory = \"path/to/your/test/data\"  # Replace with your test data path\n",
    "# evaluate_model(\"combined_model.pth\", test_data_directory)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
