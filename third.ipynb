{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 1: Imports (Consolidated) ---\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from skimage.segmentation import slic\n",
    "from skimage.color import label2rgb\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import efficientnet_v2_s, EfficientNet_V2_S_Weights, vit_b_16, ViT_B_16_Weights\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report, precision_recall_fscore_support\n",
    "\n",
    "import wandb\n",
    "import optuna  # For hyperparameter optimization\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from pytorch_grad_cam import GradCAM  # For Grad-CAM (later)\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 2: Configuration (Updated) ---\n",
    "\n",
    "# --- Dataset ---\n",
    "DATASET_ROOT = \"/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection\"  # YOUR DATASET PATH.  UPDATE THIS!\n",
    "ORIGINAL_DIR = os.path.join(DATASET_ROOT, \"Original Dataset\")\n",
    "AUGMENTED_DIR = os.path.join(DATASET_ROOT, \"Augmented Dataset\")\n",
    "\n",
    "CLASSES = [\n",
    "    \"Bacterial Blight\",\n",
    "    \"Curl Virus\",\n",
    "    \"Healthy Leaf\",\n",
    "    \"Herbicide Growth Damage\",\n",
    "    \"Leaf Hopper Jassids\",\n",
    "    \"Leaf Redding\",\n",
    "    \"Leaf Variegation\",\n",
    "]\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "CLASS_MAP = {i: name for i, name in enumerate(CLASSES)}\n",
    "\n",
    "# --- Training ---\n",
    "IMAGE_SIZE = (224, 224)   # Use 224x224\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4  # Initial learning rate (will be tuned)\n",
    "EPOCHS = 10  #  Set to a reasonable value for final training; lower for Optuna trials.\n",
    "NUM_WORKERS = 6\n",
    "VAL_SIZE = 0.2\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Model ---\n",
    "# Keep ViT settings from Phase 2\n",
    "VIT_MODEL_NAME = \"vit_b_16\"\n",
    "VIT_PRETRAINED = True\n",
    "VIT_CHECKPOINT_DIR = \"checkpoints\"  # Use the same directory\n",
    "VIT_CHECKPOINT_PATH = os.path.join(VIT_CHECKPOINT_DIR, f\"{VIT_MODEL_NAME}_best_accuracy.pth\") #From phase 2\n",
    "\n",
    "# Add EfficientNetV2 settings\n",
    "EFFNET_MODEL_NAME = \"efficientnet_v2_s\"\n",
    "EFFNET_PRETRAINED = True\n",
    "EFFNET_CHECKPOINT_DIR = \"checkpoints\"  # Use the same directory\n",
    "EFFNET_CHECKPOINT_PATH = os.path.join(EFFNET_CHECKPOINT_DIR, f\"{EFFNET_MODEL_NAME}_best.pth\") #Use best validation loss\n",
    "\n",
    "# --- Ensemble --- (NEW)\n",
    "ENSEMBLE_METHOD = \"simple\"  # \"simple\" or \"weighted\" (start with simple)\n",
    "\n",
    "# --- Hyperparameter Optimization --- (NEW)\n",
    "N_TRIALS = 10  # Number of Optuna trials (start small, increase later)\n",
    "OPTUNA_EPOCHS = 5  #  Use fewer epochs *during* Optuna trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 3: Data Loading Functions (REVISED to use data_utils.py) ---\n",
    "\n",
    "from data_utils import create_data_loaders, get_transforms, CottonDataset  # Import from data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 4: Model Definitions (ViT and EfficientNetV2) ---\n",
    "\n",
    "def get_vit_model(model_name=VIT_MODEL_NAME, pretrained=VIT_PRETRAINED, num_classes=NUM_CLASSES):\n",
    "    if model_name == \"vit_b_16\":\n",
    "        weights = ViT_B_16_Weights.DEFAULT if pretrained else None\n",
    "        model = vit_b_16(weights=weights)\n",
    "        model.heads = nn.Linear(model.heads[0].in_features, num_classes)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported ViT model name: {model_name}\")\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "\n",
    "def get_effnet_model(model_name=EFFNET_MODEL_NAME, pretrained=EFFNET_PRETRAINED, num_classes=NUM_CLASSES):\n",
    "    if model_name == \"efficientnet_v2_s\":\n",
    "        weights = EfficientNet_V2_S_Weights.DEFAULT if pretrained else None\n",
    "        model = efficientnet_v2_s(weights=weights)\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported EfficientNet model name: {model_name}\")\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "# --- Function to Load Checkpoints ---\n",
    "def load_checkpoint(model, checkpoint_path):\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        model.load_state_dict(torch.load(checkpoint_path, map_location=DEVICE))\n",
    "        print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
    "    else:\n",
    "        print(f\"Checkpoint not found at {checkpoint_path}. Starting from scratch or ImageNet weights.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 5: Generalized Training Loop (Modified for Optuna) ---\n",
    "\n",
    "def train_model(model, train_loader, val_loader, learning_rate, epochs, checkpoint_path=None, trial=None): # Added trial\n",
    "    \"\"\"\n",
    "    Trains a PyTorch model and saves checkpoints, integrated with Optuna.\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_accuracy = 0.0 # Keep track of best accuracy.\n",
    "    best_val_report = None # Initialize to None\n",
    "    precision, recall, f1 = 0.0, 0.0, 0.0  # Initialize metrics\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        val_preds = []\n",
    "        val_true = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_running_loss += loss.item() * images.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "                val_preds.extend(predicted.cpu().numpy())  # for metrics\n",
    "                val_true.extend(labels.cpu().numpy())\n",
    "\n",
    "\n",
    "        val_loss = val_running_loss / len(val_loader.dataset)\n",
    "        val_accuracy = 100 * correct_val / total_val\n",
    "\n",
    "        # Generate the classification report *dictionary*\n",
    "        report = classification_report(val_true, val_preds, target_names=CLASSES, zero_division=0, output_dict=True)\n",
    "        report = pd.DataFrame(report).transpose()\n",
    "\n",
    "\n",
    "        # Optuna reporting (report *accuracy* for maximization)\n",
    "        if trial:\n",
    "            trial.report(val_accuracy, epoch)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        # Checkpointing (save based on *accuracy*, not loss)\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_val_loss = val_loss  # Still useful to track\n",
    "            if checkpoint_path: #save only if path is given\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "                print(f\"Saved best model to {checkpoint_path}\")\n",
    "            best_val_report = report # Update with the dictionary\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(val_true, val_preds, average='weighted', zero_division=0)\n",
    "\n",
    "            # Log inside the if condition:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_accuracy\": train_accuracy,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_accuracy\": val_accuracy,\n",
    "                \"val_precision\": precision,\n",
    "                \"val_recall\": recall,\n",
    "                \"val_f1\": f1,\n",
    "            })\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "    # Check if best_val_report exists and is not None\n",
    "    if best_val_report is not None:\n",
    "        print(\"Best Validation Classification Report:\\n\", best_val_report)\n",
    "        wandb.log({\"best_validation_classification_report\": wandb.Table(dataframe=best_val_report)})\n",
    "\n",
    "    return model, best_val_accuracy  # Return accuracy for Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 6: Ensemble Prediction ---\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "def evaluate_ensemble(models, dataloader, ensemble_method=\"simple\", weights=None):\n",
    "    \"\"\"Evaluates an ensemble of models.\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    for model in models:\n",
    "        model.eval()  # Set models to evaluation mode\n",
    "        model.to(device)\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = []\n",
    "            for model in models:\n",
    "                outputs.append(model(images))\n",
    "\n",
    "            if ensemble_method == \"simple\":\n",
    "                # Simple Averaging\n",
    "                ensemble_output = torch.mean(torch.stack(outputs), dim=0)\n",
    "            elif ensemble_method == \"weighted\":\n",
    "                # Weighted Averaging\n",
    "                if weights is None:\n",
    "                    raise ValueError(\"Weights must be provided for weighted averaging.\")\n",
    "                weighted_outputs = [weights[i] * outputs[i] for i in range(len(models))]\n",
    "                ensemble_output = torch.sum(torch.stack(weighted_outputs), dim=0)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown ensemble method: {ensemble_method}\")\n",
    "\n",
    "            _, predicted = torch.max(ensemble_output, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = np.mean(np.array(all_preds) == np.array(all_labels)) * 100\n",
    "    report = classification_report(all_labels, all_preds, output_dict=True, zero_division=0) #output_dict=True is KEY\n",
    "\n",
    "    print(\"Classification Report (Raw):\")\n",
    "    print(report)  #Inspect the report\n",
    "\n",
    "    if report:\n",
    "        try:\n",
    "            df = pd.DataFrame(report).transpose()\n",
    "            ensemble_classification_report = wandb.Table(dataframe=df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating wandb table: {e}\")\n",
    "            ensemble_classification_report = None\n",
    "    else:\n",
    "        print(\"Warning: Classification report is empty.  Not creating wandb table.\")\n",
    "        ensemble_classification_report = None\n",
    "\n",
    "    precision = report['macro avg']['precision']\n",
    "    recall = report['macro avg']['recall']\n",
    "    f1 = report['macro avg']['f1-score']\n",
    "\n",
    "\n",
    "    if ensemble_classification_report:\n",
    "        wandb.log({\n",
    "            \"ensemble_accuracy\": accuracy,\n",
    "            \"ensemble_precision\": precision,\n",
    "            \"ensemble_recall\": recall,\n",
    "            \"ensemble_f1\": f1,\n",
    "            \"ensemble_classification_report\": ensemble_classification_report\n",
    "        })\n",
    "    else:\n",
    "        wandb.log({\n",
    "            \"ensemble_accuracy\": accuracy,\n",
    "            \"ensemble_precision\": precision,\n",
    "            \"ensemble_recall\": recall,\n",
    "            \"ensemble_f1\": f1,\n",
    "        })\n",
    "\n",
    "    return accuracy, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 7: Hyperparameter Optimization (Integrated) ---\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Objective function for Optuna optimization.\"\"\"\n",
    "\n",
    "    # Suggest a learning rate\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "\n",
    "    # --- Data Loading (inside objective, for Optuna) ---\n",
    "    train_transforms = get_transforms(train=True)\n",
    "    val_transforms = get_transforms(train=False)\n",
    "    train_loader, val_loader, _ = create_data_loaders(\n",
    "        ORIGINAL_DIR, train_transforms, val_transforms, BATCH_SIZE, NUM_WORKERS, CLASSES\n",
    "    )\n",
    "\n",
    "    # Get the model (optimize EfficientNet)\n",
    "    model = get_effnet_model()\n",
    "\n",
    "    # Train the model (using the suggested learning rate and Optuna integration)\n",
    "    _, val_accuracy = train_model(model, train_loader, val_loader, learning_rate=learning_rate, epochs=OPTUNA_EPOCHS, trial=trial)\n",
    "\n",
    "    return val_accuracy  # Optuna maximizes the return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 22:32:54,388] A new study created in memory with name: no-name-837c0c1b-da20-4780-b35c-21784b8cdd88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loaded File Paths (First 5 of each) ---\n",
      "Train: ['/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Bacterial Blight/BBC00155.jpg', '/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Herbicide Growth Damage/HGD00260.jpg', '/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Curl Virus/CV00202.jpg', '/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Herbicide Growth Damage/HGD00203.jpg', '/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Bacterial Blight/BBC00184.jpg']\n",
      "Validation: ['/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Herbicide Growth Damage/HGD00095.jpg', '/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Leaf Hopper Jassids/LHJ00110.jpg', '/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Leaf Redding/LR00019.jpg', '/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Curl Virus/CV00406.jpg', '/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Leaf Redding/LR00134.jpg']\n",
      "Test: ['/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Leaf Redding/LR00026.jpg', '/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Bacterial Blight/BBC00024.jpg', '/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Curl Virus/CV00174.jpg', '/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Curl Virus/CV00134.jpg', '/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Leaf Redding/LR00079.jpg']\n",
      "----------------------------------------\n",
      "--- Loaded File Paths (First 5 of each) ---\n",
      "Train: ['/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Bacterial Blight/BBC00155.jpg', '/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Herbicide Growth Damage/HGD00260.jpg', '/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Curl Virus/CV00202.jpg', '/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Herbicide Growth Damage/HGD00203.jpg', '/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Bacterial Blight/BBC00184.jpg']\n",
      "Validation: ['/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Herbicide Growth Damage/HGD00095.jpg', '/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Leaf Hopper Jassids/LHJ00110.jpg', '/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Leaf Redding/LR00019.jpg', '/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Curl Virus/CV00406.jpg', '/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Leaf Redding/LR00134.jpg']\n",
      "Test: ['/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Leaf Redding/LR00026.jpg', '/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Bacterial Blight/BBC00024.jpg', '/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Curl Virus/CV00174.jpg', '/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Curl Virus/CV00134.jpg', '/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Leaf Redding/LR00079.jpg']\n",
      "----------------------------------------\n",
      "Epoch 1/5, Train Loss: 0.6747, Train Acc: 78.14%, Val Loss: 0.3486, Val Acc: 87.62%\n",
      "Epoch 2/5, Train Loss: 0.2747, Train Acc: 91.26%, Val Loss: 0.2002, Val Acc: 93.69%\n",
      "Epoch 3/5, Train Loss: 0.1486, Train Acc: 95.00%, Val Loss: 0.1416, Val Acc: 96.03%\n",
      "Epoch 4/5, Train Loss: 0.1617, Train Acc: 95.39%, Val Loss: 0.2036, Val Acc: 94.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 22:34:24,222] Trial 0 finished with value: 96.96261682242991 and parameters: {'learning_rate': 0.0005007496855582154}. Best is trial 0 with value: 96.96261682242991.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Train Loss: 0.1030, Train Acc: 96.80%, Val Loss: 0.1278, Val Acc: 96.96%\n",
      "Finished Training\n",
      "Best Validation Classification Report:\n",
      "                          precision    recall  f1-score     support\n",
      "Bacterial Blight          0.923077  0.960000  0.941176   50.000000\n",
      "Curl Virus                0.966292  0.988506  0.977273   87.000000\n",
      "Healthy Leaf              0.983607  1.000000  0.991736   60.000000\n",
      "Herbicide Growth Damage   1.000000  0.945455  0.971963   55.000000\n",
      "Leaf Hopper Jassids       0.900000  1.000000  0.947368   36.000000\n",
      "Leaf Redding              1.000000  0.941667  0.969957  120.000000\n",
      "Leaf Variegation          0.952381  1.000000  0.975610   20.000000\n",
      "accuracy                  0.969626  0.969626  0.969626    0.969626\n",
      "macro avg                 0.960765  0.976518  0.967869  428.000000\n",
      "weighted avg              0.971227  0.969626  0.969757  428.000000\n",
      "--- Loaded File Paths (First 5 of each) ---\n",
      "Train: ['/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Bacterial Blight/BBC00155.jpg', '/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Herbicide Growth Damage/HGD00260.jpg', '/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Curl Virus/CV00202.jpg', '/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Herbicide Growth Damage/HGD00203.jpg', '/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Bacterial Blight/BBC00184.jpg']\n",
      "Validation: ['/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Herbicide Growth Damage/HGD00095.jpg', '/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Leaf Hopper Jassids/LHJ00110.jpg', '/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Leaf Redding/LR00019.jpg', '/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Curl Virus/CV00406.jpg', '/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Leaf Redding/LR00134.jpg']\n",
      "Test: ['/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Leaf Redding/LR00026.jpg', '/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Bacterial Blight/BBC00024.jpg', '/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Curl Virus/CV00174.jpg', '/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Curl Virus/CV00134.jpg', '/home/w2sg-arnav/8-phases/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset/Leaf Redding/LR00079.jpg']\n",
      "----------------------------------------\n",
      "Epoch 1/5, Train Loss: 1.7072, Train Acc: 44.89%, Val Loss: 1.2566, Val Acc: 77.34%\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 8: Main Execution ---\n",
    "\n",
    "# --- Initialize W&B ---\n",
    "if wandb.run is None:\n",
    "   run = wandb.init(project=\"vit\", entity=\"w2sgarnav\", name=\"w2sgarnav-vit-phase3\", mode=\"offline\")\n",
    "\n",
    "\n",
    "# --- Load Data (outside Optuna, for final training) ---\n",
    "#  Use standard, non-Optuna data loading for the final training.\n",
    "train_transforms = get_transforms(train=True)\n",
    "val_transforms = get_transforms(train=False)\n",
    "train_loader, val_loader, test_loader = create_data_loaders(\n",
    "    ORIGINAL_DIR, train_transforms, val_transforms, BATCH_SIZE, NUM_WORKERS, CLASSES\n",
    ")\n",
    "\n",
    "# --- 1. Hyperparameter Optimization (EfficientNet) ---\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=N_TRIALS)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value: {trial.value}\")\n",
    "print(f\"  Params: {trial.params}\")\n",
    "best_lr = trial.params[\"learning_rate\"]\n",
    "wandb.log({\"best_learning_rate\": best_lr})\n",
    "\n",
    "\n",
    "# --- 2. Train/Load Models ---\n",
    "\n",
    "# 2.1 Train/Load ViT (from Phase 2 checkpoint)\n",
    "vit_model = get_vit_model()\n",
    "load_checkpoint(vit_model, VIT_CHECKPOINT_PATH)  # Load your best ViT model\n",
    "\n",
    "# 2.2 Train EfficientNetV2 (using best LR from Optuna)\n",
    "effnet_model = get_effnet_model()\n",
    "# Train with the *best* learning rate found by Optuna.\n",
    "train_model(effnet_model, train_loader, val_loader, learning_rate=best_lr, epochs=EPOCHS, checkpoint_path=EFFNET_CHECKPOINT_PATH)\n",
    "\n",
    "\n",
    "# --- 3. Create and Evaluate Ensemble ---\n",
    "\n",
    "# 3.1 Simple Averaging\n",
    "print(\"Evaluating Simple Averaging Ensemble...\")\n",
    "ensemble_accuracy, _ = evaluate_ensemble([vit_model, effnet_model], val_loader, ensemble_method=\"simple\")\n",
    "\n",
    "\n",
    "# 3.2 Weighted Averaging (Example - you can refine the weight optimization)\n",
    "print(\"Evaluating Weighted Averaging Ensemble...\")\n",
    "#  Example weights (you can optimize these, perhaps with a separate Optuna study!)\n",
    "vit_weight = 0.6\n",
    "effnet_weight = 0.4\n",
    "ensemble_accuracy, _ = evaluate_ensemble([vit_model, effnet_model], val_loader, ensemble_method=\"weighted\", weights=[vit_weight, effnet_weight])\n",
    "\n",
    "# --- 4. Evaluate on Test Set (using the best ensemble method) ---\n",
    "print(\"Evaluating on Test Set...\")\n",
    "if ENSEMBLE_METHOD == 'simple':\n",
    "    ensemble_accuracy_test,_ = evaluate_ensemble([vit_model, effnet_model], test_loader, ensemble_method = \"simple\")\n",
    "elif ENSEMBLE_METHOD == 'weighted':\n",
    "    ensemble_accuracy_test,_ = evaluate_ensemble([vit_model, effnet_model], test_loader, ensemble_method = \"weighted\", weights = [vit_weight, effnet_weight])\n",
    "\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cotton_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
